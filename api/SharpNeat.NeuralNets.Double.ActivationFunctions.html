<!DOCTYPE html>
<!--[if IE]><![endif]-->
<html>

  <head>
    <meta charset="utf-8">
      <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
      <title>Namespace SharpNeat.NeuralNets.Double.ActivationFunctions
 </title>
      <meta name="viewport" content="width=device-width">
      <meta name="title" content="Namespace SharpNeat.NeuralNets.Double.ActivationFunctions
 ">
    
      <link rel="shortcut icon" href="../favicon.ico">
      <link rel="stylesheet" href="../styles/docfx.vendor.min.css">
      <link rel="stylesheet" href="../styles/docfx.css">
      <link rel="stylesheet" href="../styles/main.css">
      <meta property="docfx:navrel" content="../toc.html">
      <meta property="docfx:tocrel" content="toc.html">
    
    
    
  </head>
  <body data-spy="scroll" data-target="#affix" data-offset="120">
    <div id="wrapper">
      <header>

        <nav id="autocollapse" class="navbar navbar-inverse ng-scope" role="navigation">
          <div class="container">
            <div class="navbar-header">
              <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navbar">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
              </button>

              <a class="navbar-brand" href="../index.html">
                <img id="logo" class="svg" src="../logo.svg" alt="">
              </a>
            </div>
            <div class="collapse navbar-collapse" id="navbar">
              <form class="navbar-form navbar-right" role="search" id="search">
                <div class="form-group">
                  <input type="text" class="form-control" id="search-query" placeholder="Search" autocomplete="off">
                </div>
              </form>
            </div>
          </div>
        </nav>

        <div class="subnav navbar navbar-default">
          <div class="container hide-when-search" id="breadcrumb">
            <ul class="breadcrumb">
              <li></li>
            </ul>
          </div>
        </div>
      </header>
      <div role="main" class="container body-content hide-when-search">

        <div class="sidenav hide-when-search">
          <a class="btn toc-toggle collapse" data-toggle="collapse" href="#sidetoggle" aria-expanded="false" aria-controls="sidetoggle">Show / Hide Table of Contents</a>
          <div class="sidetoggle collapse" id="sidetoggle">
            <div id="sidetoc"></div>
          </div>
        </div>
        <div class="article row grid-right">
          <div class="col-md-10">
            <article class="content wrap" id="_content" data-uid="SharpNeat.NeuralNets.Double.ActivationFunctions">

  <h1 id="SharpNeat_NeuralNets_Double_ActivationFunctions" data-uid="SharpNeat.NeuralNets.Double.ActivationFunctions" class="text-break">Namespace SharpNeat.NeuralNets.Double.ActivationFunctions
</h1>
  <div class="markdown level0 summary"></div>
  <div class="markdown level0 conceptual"></div>
  <div class="markdown level0 remarks"></div>
    <h3 id="classes">Classes
</h3>
      <h4><a class="xref" href="SharpNeat.NeuralNets.Double.ActivationFunctions.ActivationFunctionMonotonicityTests.html">ActivationFunctionMonotonicityTests</a></h4>
      <section></section>
      <h4><a class="xref" href="SharpNeat.NeuralNets.Double.ActivationFunctions.ActivationFunctionOverloadTests.html">ActivationFunctionOverloadTests</a></h4>
      <section></section>
      <h4><a class="xref" href="SharpNeat.NeuralNets.Double.ActivationFunctions.ArcSinH.html">ArcSinH</a></h4>
      <section><p>The ArcSinH function (inverse hyperbolic sine function).</p>
</section>
      <h4><a class="xref" href="SharpNeat.NeuralNets.Double.ActivationFunctions.ArcTan.html">ArcTan</a></h4>
      <section><p>The ArcTan function (inverse tangent function).</p>
</section>
      <h4><a class="xref" href="SharpNeat.NeuralNets.Double.ActivationFunctions.LeakyReLU.html">LeakyReLU</a></h4>
      <section><p>Leaky rectified linear activation unit (ReLU).</p>
</section>
      <h4><a class="xref" href="SharpNeat.NeuralNets.Double.ActivationFunctions.LeakyReLUShifted.html">LeakyReLUShifted</a></h4>
      <section><p>Leaky rectified linear activation unit (ReLU).
Shifted on the x-axis so that x=0 gives y=0.5, in keeping with the logistic sigmoid.</p>
</section>
      <h4><a class="xref" href="SharpNeat.NeuralNets.Double.ActivationFunctions.Logistic.html">Logistic</a></h4>
      <section><p>The logistic function.
<a href="http://en.wikipedia.org/wiki/Logistic_function">http://en.wikipedia.org/wiki/Logistic_function</a>.</p>
</section>
      <h4><a class="xref" href="SharpNeat.NeuralNets.Double.ActivationFunctions.LogisticApproximantSteep.html">LogisticApproximantSteep</a></h4>
      <section><p>The logistic function with a steepened slope, and implemented using a fast to compute approximation of exp().
See:
<a href="https://stackoverflow.com/a/412988/15703">https://stackoverflow.com/a/412988/15703</a>
<a href="https://pdfs.semanticscholar.org/35d3/2b272879a2018a2d33d982639d4be489f789.pdf">https://pdfs.semanticscholar.org/35d3/2b272879a2018a2d33d982639d4be489f789.pdf</a> (A Fast, Compact Approximation of the Exponential Function).</p>
</section>
      <h4><a class="xref" href="SharpNeat.NeuralNets.Double.ActivationFunctions.LogisticSteep.html">LogisticSteep</a></h4>
      <section><p>The logistic function with a steepened slope.
<a href="http://en.wikipedia.org/wiki/Logistic_function">http://en.wikipedia.org/wiki/Logistic_function</a>.</p>
</section>
      <h4><a class="xref" href="SharpNeat.NeuralNets.Double.ActivationFunctions.MaxMinusOne.html">MaxMinusOne</a></h4>
      <section><p>max(-1, x,) function.</p>
</section>
      <h4><a class="xref" href="SharpNeat.NeuralNets.Double.ActivationFunctions.NullFn.html">NullFn</a></h4>
      <section><p>Null activation function. Returns zero regardless of input.</p>
</section>
      <h4><a class="xref" href="SharpNeat.NeuralNets.Double.ActivationFunctions.PolynomialApproximantSteep.html">PolynomialApproximantSteep</a></h4>
      <section><p>A very close approximation of the logistic function that avoids use of exp() and is therefore
typically much faster to compute, while giving an almost identical sigmoid curve.</p>
<p>This function was obtained from:
<a href="http://stackoverflow.com/a/34448562/15703">http://stackoverflow.com/a/34448562/15703</a></p>
<p>This might be based on the Pade approximant:
<a href="https://en.wikipedia.org/wiki/Pad%C3%A9_approximant">https://en.wikipedia.org/wiki/Pad%C3%A9_approximant</a>
<a href="https://math.stackexchange.com/a/107666">https://math.stackexchange.com/a/107666</a></p>
<p>Or perhaps the maple minimax approximation:
<a href="http://www.maplesoft.com/support/helpJP/Maple/view.aspx?path=numapprox/minimax">http://www.maplesoft.com/support/helpJP/Maple/view.aspx?path=numapprox/minimax</a></p>
<p>This is a variant that has a steeper slope at and around the origin that is intended to be a similar
slope to that of LogisticFunctionSteep.</p>
</section>
      <h4><a class="xref" href="SharpNeat.NeuralNets.Double.ActivationFunctions.QuadraticSigmoid.html">QuadraticSigmoid</a></h4>
      <section><p>A sigmoid formed by two sub-sections of the y=x^2 curve.</p>
<p>The extremes are implemented as per the leaky ReLU, i.e. there is a linear slop to
ensure there is at least a gradient to follow at the extremes.</p>
</section>
      <h4><a class="xref" href="SharpNeat.NeuralNets.Double.ActivationFunctions.ReLU.html">ReLU</a></h4>
      <section><p>Rectified linear activation unit (ReLU).</p>
</section>
      <h4><a class="xref" href="SharpNeat.NeuralNets.Double.ActivationFunctions.ScaledELU.html">ScaledELU</a></h4>
      <section><p>Scaled Exponential Linear Unit (SELU).</p>
<p>From:
Self-Normalizing Neural Networks
<a href="https://arxiv.org/abs/1706.02515">https://arxiv.org/abs/1706.02515</a></p>
<p>Original source code (including parameter values):
<a href="https://github.com/bioinf-jku/SNNs/blob/master/selu.py">https://github.com/bioinf-jku/SNNs/blob/master/selu.py</a>.</p>
</section>
      <h4><a class="xref" href="SharpNeat.NeuralNets.Double.ActivationFunctions.SoftSignSteep.html">SoftSignSteep</a></h4>
      <section><p>The softsign sigmoid.
This is a variant of softsign that has a steeper slope at and around the origin that
is intended to be a similar slope to that of LogisticFunctionSteep.</p>
</section>
      <h4><a class="xref" href="SharpNeat.NeuralNets.Double.ActivationFunctions.SReLU.html">SReLU</a></h4>
      <section><p>S-shaped rectified linear activation unit (SReLU).
From:
<a href="https://en.wikipedia.org/wiki/Activation_function">https://en.wikipedia.org/wiki/Activation_function</a>
<a href="https://arxiv.org/abs/1512.07030">https://arxiv.org/abs/1512.07030</a> [Deep Learning with S-shaped Rectified Linear Activation Units].</p>
</section>
      <h4><a class="xref" href="SharpNeat.NeuralNets.Double.ActivationFunctions.SReLUShifted.html">SReLUShifted</a></h4>
      <section><p>S-shaped rectified linear activation unit (SReLU).
Shifted on the x-axis so that x=0 gives y=0.5, in keeping with the logistic sigmoid.
From:
<a href="https://en.wikipedia.org/wiki/Activation_function">https://en.wikipedia.org/wiki/Activation_function</a>
<a href="https://arxiv.org/abs/1512.07030">https://arxiv.org/abs/1512.07030</a> [Deep Learning with S-shaped Rectified Linear Activation Units].</p>
</section>
      <h4><a class="xref" href="SharpNeat.NeuralNets.Double.ActivationFunctions.TanH.html">TanH</a></h4>
      <section><p>TanH function (hyperbolic tangent function).</p>
</section>
</article>
          </div>

          <div class="hidden-sm col-md-2" role="complementary">
            <div class="sideaffix">
              <div class="contribution">
                <ul class="nav">
                </ul>
              </div>
              <nav class="bs-docs-sidebar hidden-print hidden-xs hidden-sm affix" id="affix">
                <h5>In This Article</h5>
                <div></div>
              </nav>
            </div>
          </div>
        </div>
      </div>

      <footer>
        <div class="grad-bottom"></div>
        <div class="footer">
          <div class="container">
            <span class="pull-right">
              <a href="#top">Back to top</a>
            </span>
      
      <span>Generated by <strong>DocFX</strong></span>
          </div>
        </div>
      </footer>
    </div>

    <script type="text/javascript" src="../styles/docfx.vendor.min.js"></script>
    <script type="text/javascript" src="../styles/docfx.js"></script>
    <script type="text/javascript" src="../styles/main.js"></script>
  </body>
</html>
