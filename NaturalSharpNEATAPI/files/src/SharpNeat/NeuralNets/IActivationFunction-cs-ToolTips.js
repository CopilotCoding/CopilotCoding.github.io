NDContentPage.OnToolTipsLoaded({1836:"<div class=\"NDToolTip TClass LCSharp\"><div class=\"NDClassPrototype\" id=\"NDClassPrototype1836\"><div class=\"CPEntry TClass Current\"><div class=\"CPModifiers\"><span class=\"SHKeyword\">public sealed</span></div><div class=\"CPName\">Sine</div></div></div><div class=\"TTSummary\">Sine activation function with doubled period.</div></div>",1871:"<div class=\"NDToolTip TClass LCSharp\"><div class=\"NDClassPrototype\" id=\"NDClassPrototype1871\"><div class=\"CPEntry TClass Current\"><div class=\"CPModifiers\"><span class=\"SHKeyword\">public sealed</span></div><div class=\"CPName\">Linear</div></div></div><div class=\"TTSummary\">Linear activation function with clipping. By \'clipping\' we mean the output value is linear between x = -1 and x = 1. Below -1 and above +1 the output is clipped at -1 and +1 respectively.</div></div>",1881:"<div class=\"NDToolTip TClass LCSharp\"><div class=\"NDClassPrototype\" id=\"NDClassPrototype1881\"><div class=\"CPEntry TClass Current\"><div class=\"CPModifiers\"><span class=\"SHKeyword\">public sealed</span></div><div class=\"CPName\">Gaussian</div></div></div><div class=\"TTSummary\">Gaussian activation function. Output range is 0 to 1, that is, the tails of the Gaussian distribution curve tend towards 0 as abs(x) -&gt; Infinity and the Gaussian peak is at x = 0.</div></div>",1891:"<div class=\"NDToolTip TClass LCSharp\"><div class=\"NDClassPrototype\" id=\"NDClassPrototype1891\"><div class=\"CPEntry TClass Current\"><div class=\"CPModifiers\"><span class=\"SHKeyword\">public sealed</span></div><div class=\"CPName\">BipolarSigmoid</div></div></div><div class=\"TTSummary\">Bipolar sigmoid activation function. Output range is -1 to 1 instead of the more normal 0 to 1.</div></div>",2031:"<div class=\"NDToolTip TClass LCSharp\"><div class=\"NDClassPrototype\" id=\"NDClassPrototype2031\"><div class=\"CPEntry TClass Current\"><div class=\"CPModifiers\"><span class=\"SHKeyword\">public sealed</span></div><div class=\"CPName\">SReLUShifted</div></div></div><div class=\"TTSummary\">S-shaped rectified linear activation unit (SReLU).&nbsp; Shifted on the x-axis so that x=0 gives y=0.5, in keeping with the logistic sigmoid.&nbsp; From: https://&#8203;en&#8203;.wikipedia&#8203;.org&#8203;/wiki&#8203;/Activation_function https://&#8203;arxiv&#8203;.org&#8203;/abs&#8203;/1512&#8203;.07030 [Deep Learning with S-shaped Rectified Linear Activation Units].</div></div>",1911:"<div class=\"NDToolTip TClass LCSharp\"><div class=\"NDClassPrototype\" id=\"NDClassPrototype1911\"><div class=\"CPEntry TClass Current\"><div class=\"CPModifiers\"><span class=\"SHKeyword\">public sealed</span></div><div class=\"CPName\">BipolarGaussian</div></div></div><div class=\"TTSummary\">Bipolar Gaussian activation function. Output range is -1 to 1, that is, the tails of the Gaussian distribution curve tend towards -1 as abs(x) -&gt; Infinity and the Gaussian peak is at y = 1.</div></div>",2011:"<div class=\"NDToolTip TClass LCSharp\"><div class=\"NDClassPrototype\" id=\"NDClassPrototype2011\"><div class=\"CPEntry TClass Current\"><div class=\"CPModifiers\"><span class=\"SHKeyword\">public sealed</span></div><div class=\"CPName\">SReLU</div></div></div><div class=\"TTSummary\">S-shaped rectified linear activation unit (SReLU).&nbsp; From: https://&#8203;en&#8203;.wikipedia&#8203;.org&#8203;/wiki&#8203;/Activation_function https://&#8203;arxiv&#8203;.org&#8203;/abs&#8203;/1512&#8203;.07030 [Deep Learning with S-shaped Rectified Linear Activation Units].</div></div>",2041:"<div class=\"NDToolTip TClass LCSharp\"><div class=\"NDClassPrototype\" id=\"NDClassPrototype2041\"><div class=\"CPEntry TClass Current\"><div class=\"CPModifiers\"><span class=\"SHKeyword\">public sealed</span></div><div class=\"CPName\">ReLU</div></div></div><div class=\"TTSummary\">Rectified linear activation unit (ReLU).</div></div>",2051:"<div class=\"NDToolTip TClass LCSharp\"><div class=\"NDClassPrototype\" id=\"NDClassPrototype2051\"><div class=\"CPEntry TClass Current\"><div class=\"CPModifiers\"><span class=\"SHKeyword\">public sealed</span></div><div class=\"CPName\">QuadraticSigmoid</div></div></div><div class=\"TTSummary\">A sigmoid formed by two sub-sections of the y=x^2 curve.&nbsp; The extremes are implemented as per the leaky ReLU, i.e. there is a linear slop to ensure there is at least a gradient to follow at the extremes.</div></div>",2081:"<div class=\"NDToolTip TClass LCSharp\"><div class=\"NDClassPrototype\" id=\"NDClassPrototype2081\"><div class=\"CPEntry TClass Current\"><div class=\"CPModifiers\"><span class=\"SHKeyword\">public sealed</span></div><div class=\"CPName\">PolynomialApproximantSteep</div></div></div><div class=\"TTSummary\">A very close approximation of the logistic function that avoids use of exp() and is therefore typically much faster to compute, while giving an almost identical sigmoid curve.&nbsp; This function was obtained from: http://&#8203;stackoverflow&#8203;.com&#8203;/a&#8203;/34448562&#8203;/15703&nbsp; This might be based on the Pade approximant: https://&#8203;en&#8203;.wikipedia&#8203;.org&#8203;/wiki&#8203;/Pad%C3%A9_approximant https://&#8203;math&#8203;.stackexchange&#8203;.com&#8203;/a&#8203;/107666&nbsp; Or perhaps the maple minimax approximation: http://&#8203;www&#8203;.maplesoft&#8203;.com&#8203;/support&#8203;/helpJP&#8203;/Maple&#8203;/view&#8203;.aspx&#8203;?path=numapprox&#8203;/minimax&nbsp; This is a variant that has a steeper slope at and around the origin that is intended to be a similar slope to that of LogisticFunctionSteep.</div></div>",2021:"<div class=\"NDToolTip TClass LCSharp\"><div class=\"NDClassPrototype\" id=\"NDClassPrototype2021\"><div class=\"CPEntry TClass Current\"><div class=\"CPModifiers\"><span class=\"SHKeyword\">public sealed</span></div><div class=\"CPName\">SoftSignSteep</div></div></div><div class=\"TTSummary\">The softsign sigmoid.&nbsp; This is a variant of softsign that has a steeper slope at and around the origin that is intended to be a similar slope to that of LogisticFunctionSteep.</div></div>",2121:"<div class=\"NDToolTip TClass LCSharp\"><div class=\"NDClassPrototype\" id=\"NDClassPrototype2121\"><div class=\"CPEntry TClass Current\"><div class=\"CPModifiers\"><span class=\"SHKeyword\">public sealed</span></div><div class=\"CPName\">LeakyReLUShifted</div></div></div><div class=\"TTSummary\">Leaky rectified linear activation unit (ReLU).&nbsp; Shifted on the x-axis so that x=0 gives y=0.5, in keeping with the logistic sigmoid.</div></div>",2101:"<div class=\"NDToolTip TClass LCSharp\"><div class=\"NDClassPrototype\" id=\"NDClassPrototype2101\"><div class=\"CPEntry TClass Current\"><div class=\"CPModifiers\"><span class=\"SHKeyword\">public sealed</span></div><div class=\"CPName\">MaxMinusOne</div></div></div><div class=\"TTSummary\">max(-1, x,) function.</div></div>",2142:"<div class=\"NDToolTip TClass LCSharp\"><div class=\"NDClassPrototype\" id=\"NDClassPrototype2142\"><div class=\"CPEntry TClass Current\"><div class=\"CPModifiers\"><span class=\"SHKeyword\">public sealed</span></div><div class=\"CPName\">LeakyReLU</div></div></div><div class=\"TTSummary\">Leaky rectified linear activation unit (ReLU).</div></div>",2001:"<div class=\"NDToolTip TClass LCSharp\"><div class=\"NDClassPrototype\" id=\"NDClassPrototype2001\"><div class=\"CPEntry TClass Current\"><div class=\"CPModifiers\"><span class=\"SHKeyword\">public sealed</span></div><div class=\"CPName\">TanH</div></div></div><div class=\"TTSummary\">TanH function (hyperbolic tangent function).</div></div>",2061:"<div class=\"NDToolTip TClass LCSharp\"><div class=\"NDClassPrototype\" id=\"NDClassPrototype2061\"><div class=\"CPEntry TClass Current\"><div class=\"CPModifiers\"><span class=\"SHKeyword\">public sealed</span></div><div class=\"CPName\">ScaledELU</div></div></div><div class=\"TTSummary\">Scaled Exponential Linear Unit (SELU).&nbsp; From: Self-Normalizing Neural Networks https://&#8203;arxiv&#8203;.org&#8203;/abs&#8203;/1706&#8203;.02515&nbsp; Original source code (including parameter values): https://&#8203;github&#8203;.com&#8203;/bioinf-jku&#8203;/SNNs&#8203;/blob&#8203;/master&#8203;/selu&#8203;.py.</div></div>",2071:"<div class=\"NDToolTip TClass LCSharp\"><div class=\"NDClassPrototype\" id=\"NDClassPrototype2071\"><div class=\"CPEntry TClass Current\"><div class=\"CPModifiers\"><span class=\"SHKeyword\">public sealed</span></div><div class=\"CPName\">NullFn</div></div></div><div class=\"TTSummary\">Null activation function. Returns zero regardless of input.</div></div>",2091:"<div class=\"NDToolTip TClass LCSharp\"><div class=\"NDClassPrototype\" id=\"NDClassPrototype2091\"><div class=\"CPEntry TClass Current\"><div class=\"CPModifiers\"><span class=\"SHKeyword\">public sealed</span></div><div class=\"CPName\">LogisticSteep</div></div></div><div class=\"TTSummary\">The logistic function with a steepened slope.&nbsp; http://&#8203;en&#8203;.wikipedia&#8203;.org&#8203;/wiki&#8203;/Logistic_function.</div></div>",2111:"<div class=\"NDToolTip TClass LCSharp\"><div class=\"NDClassPrototype\" id=\"NDClassPrototype2111\"><div class=\"CPEntry TClass Current\"><div class=\"CPModifiers\"><span class=\"SHKeyword\">public sealed</span></div><div class=\"CPName\">Logistic</div></div></div><div class=\"TTSummary\">The logistic function.&nbsp; http://&#8203;en&#8203;.wikipedia&#8203;.org&#8203;/wiki&#8203;/Logistic_function.</div></div>",2131:"<div class=\"NDToolTip TClass LCSharp\"><div class=\"NDClassPrototype\" id=\"NDClassPrototype2131\"><div class=\"CPEntry TClass Current\"><div class=\"CPModifiers\"><span class=\"SHKeyword\">public sealed</span></div><div class=\"CPName\">LogisticApproximantSteep</div></div></div><div class=\"TTSummary\">The logistic function with a steepened slope, and implemented using a fast to compute approximation of exp().&nbsp; See: https://&#8203;stackoverflow&#8203;.com&#8203;/a&#8203;/412988&#8203;/15703 https://&#8203;pdfs&#8203;.semanticscholar&#8203;.org&#8203;/35d3&#8203;/2b272879a2018a2d33d982639d4be489f78&#8203;9&#8203;.pdf (A Fast, Compact Approximation of the Exponential Function).</div></div>",2152:"<div class=\"NDToolTip TClass LCSharp\"><div class=\"NDClassPrototype\" id=\"NDClassPrototype2152\"><div class=\"CPEntry TClass Current\"><div class=\"CPModifiers\"><span class=\"SHKeyword\">public sealed</span></div><div class=\"CPName\">ArcTan</div></div></div><div class=\"TTSummary\">The ArcTan function (inverse tangent function).</div></div>",2162:"<div class=\"NDToolTip TClass LCSharp\"><div class=\"NDClassPrototype\" id=\"NDClassPrototype2162\"><div class=\"CPEntry TClass Current\"><div class=\"CPModifiers\"><span class=\"SHKeyword\">public sealed</span></div><div class=\"CPName\">ArcSinH</div></div></div><div class=\"TTSummary\">The ArcSinH function (inverse hyperbolic sine function).</div></div>"});